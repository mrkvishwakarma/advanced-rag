{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search\n",
    "\n",
    "Hybrid search combines traditional keyword-based search with semantic search to provide more accurate and relevant results. In the RAG application, it facilitates the discovery of relevant research articles based on user queries by integrating keyword-based search with semantic search capabilities. This integration enables the application to retrieve articles that match both keywords and semantic meaning, making it particularly useful for handling complex queries involving nuanced concepts, synonyms, and related ideas.\n",
    "\n",
    "![Hybrid Search](images/Hybrid_Search.png)\n",
    "\n",
    "\n",
    "In this notebook, we will delve into the implementation details of the hybrid search approach in the RAG application, exploring how it leverages both keyword-based and semantic search techniques to provide a more effective search experience.\n",
    "\n",
    "Here are the steps:\n",
    "* [Loading chunked dataset](#loading-the-chunks-from-the-previous-steps)\n",
    "* [Sparse Index](#Hybrid-Search---Sparse-Index)\n",
    "* [Dense Index](#hybrid-search---dense-index)\n",
    "* [Merging Results](#hybrid-search---merging-results)\n",
    "* [Generating a reply with merged results](#using-merged-results-to-generate-a-reply)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual improvements\n",
    "\n",
    "We will use [rich library](https://github.com/Textualize/rich) to make the output more readable, and supress warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich_theme_manager import Theme, ThemeManager\n",
    "import pathlib\n",
    "\n",
    "theme_dir = pathlib.Path(\"themes\")\n",
    "theme_manager = ThemeManager(theme_dir=theme_dir)\n",
    "dark = theme_manager.get(\"dark\")\n",
    "\n",
    "# Create a console with the dark theme\n",
    "console = Console(theme=dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search - Sparse Index\n",
    "\n",
    "We will use bm25 supported database to complement the semantic search with the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "from bm25s.tokenization import Tokenizer, Tokenized\n",
    "import Stemmer  # optional: for stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the chunks from the previous steps\n",
    "\n",
    "We will use the chunks from the AI Arxiv dataset, we used before. These chunks were split using semantic chunking and enriched with context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "corpus_json = json.load(open('data/corpus.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Sparse Index\n",
    "\n",
    "We will use an in-memory index using BM25. Many (vector) databases support BM25 natively, and many others support indexing and searching on calculated sparse vectors.\n",
    "\n",
    "In this example, we will also define a stemmer and stop-words to clean up the text and better select the tokens/terms that will be indexed in the sparse index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = [doc[\"text\"] for doc in corpus_json]\n",
    "\n",
    "# optional: create a stemmer\n",
    "english_stemmer = Stemmer.Stemmer(\"english\")\n",
    "\n",
    "# Initialize the Tokenizer with the stemmer\n",
    "sparse_tokenizer = Tokenizer(\n",
    "    stemmer=english_stemmer,\n",
    "    lower=True, # lowercase the tokens\n",
    "    stopwords=\"english\",  # or pass a list of stopwords\n",
    "    splitter=r\"\\w+\",  # by default r\"(?u)\\b\\w\\w+\\b\", can also be a function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'a'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'an'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'and'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'are'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'as'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'at'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'be'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'but'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'by'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'for'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'if'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'in'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'into'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'is'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'it'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'no'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'not'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'of'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'on'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'or'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'such'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'that'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'the'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'their'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'then'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'there'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'these'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'they'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'this'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'to'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'was'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'will'</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'with'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[92m'a'\u001b[0m,\n",
       "    \u001b[92m'an'\u001b[0m,\n",
       "    \u001b[92m'and'\u001b[0m,\n",
       "    \u001b[92m'are'\u001b[0m,\n",
       "    \u001b[92m'as'\u001b[0m,\n",
       "    \u001b[92m'at'\u001b[0m,\n",
       "    \u001b[92m'be'\u001b[0m,\n",
       "    \u001b[92m'but'\u001b[0m,\n",
       "    \u001b[92m'by'\u001b[0m,\n",
       "    \u001b[92m'for'\u001b[0m,\n",
       "    \u001b[92m'if'\u001b[0m,\n",
       "    \u001b[92m'in'\u001b[0m,\n",
       "    \u001b[92m'into'\u001b[0m,\n",
       "    \u001b[92m'is'\u001b[0m,\n",
       "    \u001b[92m'it'\u001b[0m,\n",
       "    \u001b[92m'no'\u001b[0m,\n",
       "    \u001b[92m'not'\u001b[0m,\n",
       "    \u001b[92m'of'\u001b[0m,\n",
       "    \u001b[92m'on'\u001b[0m,\n",
       "    \u001b[92m'or'\u001b[0m,\n",
       "    \u001b[92m'such'\u001b[0m,\n",
       "    \u001b[92m'that'\u001b[0m,\n",
       "    \u001b[92m'the'\u001b[0m,\n",
       "    \u001b[92m'their'\u001b[0m,\n",
       "    \u001b[92m'then'\u001b[0m,\n",
       "    \u001b[92m'there'\u001b[0m,\n",
       "    \u001b[92m'these'\u001b[0m,\n",
       "    \u001b[92m'they'\u001b[0m,\n",
       "    \u001b[92m'this'\u001b[0m,\n",
       "    \u001b[92m'to'\u001b[0m,\n",
       "    \u001b[92m'was'\u001b[0m,\n",
       "    \u001b[92m'will'\u001b[0m,\n",
       "    \u001b[92m'with'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(sparse_tokenizer.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    }
   ],
   "source": [
    "# Tokenize the corpus and only keep the ids (faster and saves memory)\n",
    "corpus_sparse_tokens = (\n",
    "    sparse_tokenizer\n",
    "    .tokenize(\n",
    "        corpus_text, \n",
    "        update_vocab=True, # update the vocab as we tokenize\n",
    "        return_as=\"ids\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the BM25 retriever and attach your corpus_json to it\n",
    "sparse_index = bm25s.BM25(corpus=corpus_json)\n",
    "# Now, index the corpus_tokens (the corpus_json is not used yet)\n",
    "sparse_index.index(corpus_sparse_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The tokenizer vocabulary includes <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1689</span> tokens/terms\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The tokenizer vocabulary includes \u001b[91m1689\u001b[0m tokens/terms\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The index of the context is <span style=\"color: #ff0000; text-decoration-color: #ff0000\">127</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The index of the context is \u001b[91m127\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_dict = sparse_tokenizer.get_vocab_dict()\n",
    "console.print(f\"The tokenizer vocabulary includes {len(vocab_dict)} tokens/terms\")\n",
    "\n",
    "focus_token = 'context'\n",
    "focus_token_index = vocab_dict.get(focus_token)\n",
    "console.print(f\"The index of the {focus_token} is {focus_token_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer can encode (convert the text into ids) and decode (convert the ids back into text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'context'</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[92m'context'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(sparse_tokenizer.decode([[focus_token_index]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Sparse Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'data'</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00\">array</span><span style=\"font-weight: bold\">([</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.738586</span>  , <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.75115275</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.0805568</span> , <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.6571838</span> , <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.6571838</span> ,\n",
       "       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.6571838</span> <span style=\"font-weight: bold\">]</span>, <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">dtype</span>=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">float32</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'indices'</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00\">array</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">9</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000\">10</span>, <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000\">45</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000\">45</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000\">45</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">dtype</span>=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">int32</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'indptr'</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00\">array</span><span style=\"font-weight: bold\">([</span>   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">12</span>,   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">33</span>, <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000\">4037</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000\">4038</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000\">4039</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">dtype</span>=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">int32</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'num_docs'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">46</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[92m'data'\u001b[0m: \u001b[93marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[91m0.738586\u001b[0m  , \u001b[91m0.75115275\u001b[0m, \u001b[91m1.0805568\u001b[0m , \u001b[33m...\u001b[0m, \u001b[91m1.6571838\u001b[0m , \u001b[91m1.6571838\u001b[0m ,\n",
       "       \u001b[91m1.6571838\u001b[0m \u001b[1m]\u001b[0m, \u001b[1;38;2;232;125;62mdtype\u001b[0m=\u001b[94mfloat32\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[92m'indices'\u001b[0m: \u001b[93marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[91m0\u001b[0m,  \u001b[91m9\u001b[0m, \u001b[91m10\u001b[0m, \u001b[33m...\u001b[0m, \u001b[91m45\u001b[0m, \u001b[91m45\u001b[0m, \u001b[91m45\u001b[0m\u001b[1m]\u001b[0m, \u001b[1;38;2;232;125;62mdtype\u001b[0m=\u001b[94mint32\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[92m'indptr'\u001b[0m: \u001b[93marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m   \u001b[91m0\u001b[0m,   \u001b[91m12\u001b[0m,   \u001b[91m33\u001b[0m, \u001b[33m...\u001b[0m, \u001b[91m4037\u001b[0m, \u001b[91m4038\u001b[0m, \u001b[91m4039\u001b[0m\u001b[1m]\u001b[0m, \u001b[1;38;2;232;125;62mdtype\u001b[0m=\u001b[94mint32\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[92m'num_docs'\u001b[0m: \u001b[91m46\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(sparse_index.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token, the index holds the list of documents (chunks) that include it, and the score of that token in that document (chunk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Index of the token `context` in the BM25 retriever: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">127</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Index of the token `context` in the BM25 retriever: \u001b[91m127\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">    Document Scores for     </span>\n",
       "<span style=\"font-style: italic\">         `context`          </span>\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Document ID </span>┃<span style=\"font-weight: bold\">      Score </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           0 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.4434834 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           2 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.7355117 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           3 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 0.53710693 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           4 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 0.90847206 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          13 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.5116058 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          14 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.9670208 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; background-color: #808000\">          15 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; background-color: #808000\">  1.1056415 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          30 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.7444794 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          37 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.6708646 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          41 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.7355117 </span>│\n",
       "└─────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m    Document Scores for     \u001b[0m\n",
       "\u001b[3m         `context`          \u001b[0m\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mDocument ID\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Score\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          0\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.4434834\u001b[0m\u001b[92m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m          2\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.7355117\u001b[0m\u001b[92m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m          3\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m0.53710693\u001b[0m\u001b[92m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m          4\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m0.90847206\u001b[0m\u001b[92m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         13\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.5116058\u001b[0m\u001b[92m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         14\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.9670208\u001b[0m\u001b[92m \u001b[0m│\n",
       "│\u001b[36;43m \u001b[0m\u001b[36;43m         15\u001b[0m\u001b[36;43m \u001b[0m│\u001b[92;43m \u001b[0m\u001b[92;43m 1.1056415\u001b[0m\u001b[92;43m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         30\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.7444794\u001b[0m\u001b[92m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         37\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.6708646\u001b[0m\u001b[92m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         41\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.7355117\u001b[0m\u001b[92m \u001b[0m│\n",
       "└─────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.table import Table\n",
    "from rich.style import Style\n",
    "\n",
    "token_index = vocab_dict.get(focus_token)\n",
    "console.print(f\"Index of the token `{focus_token}` in the BM25 retriever: {token_index}\")\n",
    "score_index = sparse_index.scores.get('indptr')[token_index]\n",
    "next_score_index = sparse_index.scores.get('indptr')[token_index+1]\n",
    "\n",
    "table = Table(title=f\"Document Scores for `{focus_token}`\")\n",
    "\n",
    "table.add_column(\"Document ID\", justify=\"right\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(\"Score\", justify=\"right\", style=\"bright_green\")\n",
    "\n",
    "max_score = max(sparse_index.scores['data'][score_index:next_score_index])\n",
    "# Define styles for specific rows\n",
    "highlight_style = Style(bgcolor=\"yellow\")\n",
    "\n",
    "for i in range(score_index, next_score_index):\n",
    "    doc_id = sparse_index.scores['indices'][i]\n",
    "    doc_score = sparse_index.scores['data'][i]\n",
    "    if doc_score == max_score:\n",
    "        table.add_row(\n",
    "            str(doc_id),\n",
    "            str(doc_score), style=highlight_style\n",
    "        )\n",
    "    else:\n",
    "        table.add_row(\n",
    "            str(doc_id),\n",
    "            str(doc_score)\n",
    "        )\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching the Sparse Index\n",
    "\n",
    "As we are doing in the dense index, we need to tokenize and encode the query text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">127</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000\">128</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000\">15</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[91m127\u001b[0m, \u001b[91m128\u001b[0m, \u001b[91m15\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query the corpus\n",
    "query = \"What is context size of Mixtral?\"\n",
    "query_tokens = (\n",
    "    sparse_tokenizer\n",
    "    .tokenize(\n",
    "        [query], \n",
    "        update_vocab=False, \n",
    "        return_as=\"ids\"\n",
    "    )\n",
    ")\n",
    "\n",
    "console.print(query_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the encoded query to search the sparse index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.99</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'expertsâ </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> to process the token and combine their output additively. This </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">technique increases the number of parameters of a model while controlling cost and latency, as the model only uses </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">describes the key architectural details of the Mixtral model, a sparse mixture-of-experts language model that </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">outperforms larger models like Llama 2 70B and GPT-3.5 on various benchmarks.'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m1\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m1.99\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'id'\u001b[0m: \u001b[91m2\u001b[0m, \u001b[92m'text'\u001b[0m: \u001b[92m'expertsâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m to process the token and combine their output additively. This \u001b[0m\n",
       "\u001b[92mtechnique increases the number of parameters of a model while controlling cost and latency, as the model only uses \u001b[0m\n",
       "\u001b[92ma fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context \u001b[0m\n",
       "\u001b[92msize of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several \u001b[0m\n",
       "\u001b[92mbenchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk \u001b[0m\n",
       "\u001b[92mdescribes the key architectural details of the Mixtral model, a sparse mixture-of-experts language model that \u001b[0m\n",
       "\u001b[92moutperforms larger models like Llama 2 70B and GPT-3.5 on various benchmarks.'\u001b[0m, \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of \u001b[0m\n",
       "\u001b[92mExperts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.86</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">14</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4%</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0% Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Italian. # 3.2 Long range performance To assess the capabilities of Mixtral to tackle long context, we evaluate it </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">on the passkey retrieval task introduced in </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">23</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, a synthetic task designed to measure the ability of the model to </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Left</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> show that Mixtral achieves a 100%</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Right</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">shows that the perplexity of Mixtral on a subset of the proof-pile dataset </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">2</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> decreases monotonically as the size </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">of the context increases. Passkey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Loc\\n\\nThe chunk discusses Mixtral's performance on multilingual benchmarks and its ability to handle long-range </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">context, demonstrating its strong capabilities in these areas.\"</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, \n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m2\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m1.86\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'id'\u001b[0m: \u001b[91m14\u001b[0m, \u001b[92m'text'\u001b[0m: \u001b[92m\"Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish \u001b[0m\n",
       "\u001b[92mArc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4%\u001b[0m\n",
       "\u001b[92m70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% \u001b[0m\n",
       "\u001b[92m64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0% Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On \u001b[0m\n",
       "\u001b[92mARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and \u001b[0m\n",
       "\u001b[92mItalian. # 3.2 Long range performance To assess the capabilities of Mixtral to tackle long context, we evaluate it \u001b[0m\n",
       "\u001b[92mon the passkey retrieval task introduced in \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m23\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m, a synthetic task designed to measure the ability of the model to \u001b[0m\n",
       "\u001b[92mretrieve a passkey inserted randomly in a long prompt. Results in Figure 4 \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mLeft\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m show that Mixtral achieves a 100%\u001b[0m\n",
       "\u001b[92mretrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mRight\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m \u001b[0m\n",
       "\u001b[92mshows that the perplexity of Mixtral on a subset of the proof-pile dataset \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m2\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m decreases monotonically as the size \u001b[0m\n",
       "\u001b[92mof the context increases. Passkey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey \u001b[0m\n",
       "\u001b[92mLoc\\n\\nThe chunk discusses Mixtral's performance on multilingual benchmarks and its ability to handle long-range \u001b[0m\n",
       "\u001b[92mcontext, demonstrating its strong capabilities in these areas.\"\u001b[0m, \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \n",
       "\u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">3</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.46</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'chat model on human bench- marks. Both the base and instruct models are </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">of experts model </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">parameters. At every layer, for every token, a router network chooses two of these groups </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">the â\\n\\nThis chunk </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">most benchmarks. It describes the key architectural details of Mixtral, including its use of a sparse </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mixture-of-experts network, and mentions that the base and instruct models are released under the Apache 2.0 </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">license.'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m3\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m1.46\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'id'\u001b[0m: \u001b[91m1\u001b[0m, \u001b[92m'text'\u001b[0m: \u001b[92m'chat model on human bench- marks. Both the base and instruct models are \u001b[0m\n",
       "\u001b[92mreleased under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: \u001b[0m\n",
       "\u001b[92mhttps://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture\u001b[0m\n",
       "\u001b[92mof experts model \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 \u001b[0m\n",
       "\u001b[92mon most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference \u001b[0m\n",
       "\u001b[92mspeed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts \u001b[0m\n",
       "\u001b[92mnetwork. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of \u001b[0m\n",
       "\u001b[92mparameters. At every layer, for every token, a router network chooses two of these groups \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mthe â\\n\\nThis chunk \u001b[0m\n",
       "\u001b[92mintroduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on \u001b[0m\n",
       "\u001b[92mmost benchmarks. It describes the key architectural details of Mixtral, including its use of a sparse \u001b[0m\n",
       "\u001b[92mmixture-of-experts network, and mentions that the base and instruct models are released under the Apache 2.0 \u001b[0m\n",
       "\u001b[92mlicense.'\u001b[0m, \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">4</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.33</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">15</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"3.8 â Mixtral_8x7B 3.5 32 &gt; $3.0 i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 228 fos a 2.0 0 5k 10k 15k 20k 25k </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">30k Context length Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 &gt; 0.6 $3.0 i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 228 04 fos 0.2 a 2.0 0.0 OK </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixtral. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Left</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Mixtral has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">and length of the input sequence. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Right</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> The perplexity of Mixtral on the proof-pile dataset decreases </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">monotonically as the context length increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">model, demonstrating its ability to retrieve a passkey regardless of its location in a long input sequence, and </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">showing that the model's perplexity on the proof-pile dataset decreases as the context length increases.\"</span>, \n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m4\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m1.33\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'id'\u001b[0m: \u001b[91m15\u001b[0m, \u001b[92m'text'\u001b[0m: \u001b[92m\"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m 228 fos a 2.0 0 5k 10k 15k 20k 25k \u001b[0m\n",
       "\u001b[92m30k Context length Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m 228 04 fos 0.2 a 2.0 0.0 OK \u001b[0m\n",
       "\u001b[92m4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of \u001b[0m\n",
       "\u001b[92mMixtral. \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mLeft\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m Mixtral has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey \u001b[0m\n",
       "\u001b[92mand length of the input sequence. \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mRight\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m The perplexity of Mixtral on the proof-pile dataset decreases \u001b[0m\n",
       "\u001b[92mmonotonically as the context length increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral \u001b[0m\n",
       "\u001b[92mmodel, demonstrating its ability to retrieve a passkey regardless of its location in a long input sequence, and \u001b[0m\n",
       "\u001b[92mshowing that the model's perplexity on the proof-pile dataset decreases as the context length increases.\"\u001b[0m, \n",
       "\u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">5</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.32</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'4 2 0 2 n a J 8 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> G L . s c </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> language model. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">blocks </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i.e. experts</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. For every token, at each layer, a router network selects two experts to process the current </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">state and combine their outputs. Even though each token only sees two experts, the selected experts can be </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">and GPT-3.5 on various benchmarks. It also describes the model architecture and the fine-tuned Mixtral 8x7B - </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Instruct model.'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: \n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m5\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m1.32\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'id'\u001b[0m: \u001b[91m0\u001b[0m, \u001b[92m'text'\u001b[0m: \u001b[92m'4 2 0 2 n a J 8 \u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m G L . s c \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # \u001b[0m\n",
       "\u001b[92mMixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris \u001b[0m\n",
       "\u001b[92mBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume \u001b[0m\n",
       "\u001b[92mBour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep \u001b[0m\n",
       "\u001b[92mSubramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e\u001b[0m\n",
       "\u001b[92mLacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m language model. \u001b[0m\n",
       "\u001b[92mMixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward \u001b[0m\n",
       "\u001b[92mblocks \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mi.e. experts\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. For every token, at each layer, a router network selects two experts to process the current \u001b[0m\n",
       "\u001b[92mstate and combine their outputs. Even though each token only sees two experts, the selected experts can be \u001b[0m\n",
       "\u001b[92mdifferent at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active \u001b[0m\n",
       "\u001b[92mparameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches \u001b[0m\n",
       "\u001b[92mLlama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on \u001b[0m\n",
       "\u001b[92mmathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow \u001b[0m\n",
       "\u001b[92minstructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B \u001b[0m\n",
       "\u001b[92mâ\\n\\nThis chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B \u001b[0m\n",
       "\u001b[92mand GPT-3.5 on various benchmarks. It also describes the model architecture and the fine-tuned Mixtral 8x7B - \u001b[0m\n",
       "\u001b[92mInstruct model.'\u001b[0m, \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \n",
       "\u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">6</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.24</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">12</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Size and Efficiency. We compare our performance to the Llama 2 family, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">see Figure 3</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. As a sparse </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixtral is able to outperform Llama 2 70B across most categories. Note that this analysis focuses on the active </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">parameter count </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">see Section 2.1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, which is directly proportional to the inference compute cost, but does not </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">running more than one expert per device. They are more suitable for batched workloads where one can reach a good </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">degree of arithmetic intensity. Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">47B tokens </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">compared to 70B</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">gpt-3.5-turbo-1106. 2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n\\nThis chunk </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">discusses the size and efficiency of the Mixtral model, comparing its performance to the Llama 2 family of models. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">It highlights that Mixtral, as a sparse mixture-of-experts model, uses significantly fewer active parameters than </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Llama 2 70B while outperforming it across most benchmarks. The chunk also compares the performance of Mixtral 8x7B </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">to Llama 2 70B and GPT-3.5.'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: \n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m6\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m1.24\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'id'\u001b[0m: \u001b[91m12\u001b[0m, \u001b[92m'text'\u001b[0m: \u001b[92m'Size and Efficiency. We compare our performance to the Llama 2 family, \u001b[0m\n",
       "\u001b[92maiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92msee Figure 3\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. As a sparse \u001b[0m\n",
       "\u001b[92mMixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters,\u001b[0m\n",
       "\u001b[92mMixtral is able to outperform Llama 2 70B across most categories. Note that this analysis focuses on the active \u001b[0m\n",
       "\u001b[92mparameter count \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92msee Section 2.1\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m, which is directly proportional to the inference compute cost, but does not \u001b[0m\n",
       "\u001b[92mconsider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its \u001b[0m\n",
       "\u001b[92msparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the \u001b[0m\n",
       "\u001b[92mSMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when \u001b[0m\n",
       "\u001b[92mrunning more than one expert per device. They are more suitable for batched workloads where one can reach a good \u001b[0m\n",
       "\u001b[92mdegree of arithmetic intensity. Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of \u001b[0m\n",
       "\u001b[92mMixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other\u001b[0m\n",
       "\u001b[92mmodels. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m47B tokens \u001b[0m\n",
       "\u001b[92mcompared to 70B\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, \u001b[0m\n",
       "\u001b[92mgpt-3.5-turbo-1106. 2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n\\nThis chunk \u001b[0m\n",
       "\u001b[92mdiscusses the size and efficiency of the Mixtral model, comparing its performance to the Llama 2 family of models. \u001b[0m\n",
       "\u001b[92mIt highlights that Mixtral, as a sparse mixture-of-experts model, uses significantly fewer active parameters than \u001b[0m\n",
       "\u001b[92mLlama 2 70B while outperforming it across most benchmarks. The chunk also compares the performance of Mixtral 8x7B \u001b[0m\n",
       "\u001b[92mto Llama 2 70B and GPT-3.5.'\u001b[0m, \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \n",
       "\u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">7</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.12</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">4</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"Instruct under the Apache 2.0 license1, free for academic and commercial </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Architectural details Mixtral is based on a transformer architecture </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">31</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> and uses the same modifications as </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">described in </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">18</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">and the feed- forward blocks are replaced by Mixture-of-Expert layers </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Section 2.1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. The model architecture </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">parameters are summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">model, including its use of a transformer architecture with a 32k token context length and mixture-of-expert </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">layers. It also mentions the model's open-source licensing and deployment options.\"</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m7\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m1.12\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'id'\u001b[0m: \u001b[91m4\u001b[0m, \u001b[92m'text'\u001b[0m: \u001b[92m\"Instruct under the Apache 2.0 license1, free for academic and commercial \u001b[0m\n",
       "\u001b[92musage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral \u001b[0m\n",
       "\u001b[92mwith a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels \u001b[0m\n",
       "\u001b[92mfor efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 \u001b[0m\n",
       "\u001b[92mArchitectural details Mixtral is based on a transformer architecture \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m31\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m and uses the same modifications as \u001b[0m\n",
       "\u001b[92mdescribed in \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m18\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m, with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, \u001b[0m\n",
       "\u001b[92mand the feed- forward blocks are replaced by Mixture-of-Expert layers \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSection 2.1\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. The model architecture \u001b[0m\n",
       "\u001b[92mparameters are summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language \u001b[0m\n",
       "\u001b[92mmodel, including its use of a transformer architecture with a 32k token context length and mixture-of-expert \u001b[0m\n",
       "\u001b[92mlayers. It also mentions the model's open-source licensing and deployment options.\"\u001b[0m, \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral\u001b[0m\n",
       "\u001b[92mof Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">8</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.89</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">41</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Hellaswag: Can a machine really finish your sentence? arXiv preprint </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">arXiv:1905.07830, 2019. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">33</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">preprint arXiv:2306.05685, 2023.\\n\\nThe chunk discusses two references related to language model benchmarking, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">including the Hellaswag dataset and the MT-Bench and Chatbot Arena benchmarks. This is situated within the broader </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">context of the paper, which introduces the Mixtral language model and evaluates its performance on various </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">benchmarks.'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m8\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m0.89\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'id'\u001b[0m: \u001b[91m41\u001b[0m, \u001b[92m'text'\u001b[0m: \u001b[92m'Hellaswag: Can a machine really finish your sentence? arXiv preprint \u001b[0m\n",
       "\u001b[92marXiv:1905.07830, 2019. \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m33\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\u001b[0m\n",
       "\u001b[92mZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv \u001b[0m\n",
       "\u001b[92mpreprint arXiv:2306.05685, 2023.\\n\\nThe chunk discusses two references related to language model benchmarking, \u001b[0m\n",
       "\u001b[92mincluding the Hellaswag dataset and the MT-Bench and Chatbot Arena benchmarks. This is situated within the broader \u001b[0m\n",
       "\u001b[92mcontext of the paper, which introduces the Mixtral language model and evaluates its performance on various \u001b[0m\n",
       "\u001b[92mbenchmarks.'\u001b[0m, \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">9</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.80</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">3</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the information in the sequence. We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">instructions using supervised fine-tuning and Direct Preference Optimization </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">25</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. Its performance notably </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â\\n\\nThis chunk describes the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixture of Experts layer architecture used in the Mixtral model, as well as the superior performance of Mixtral </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">compared to other models on various benchmarks, including mathematics, code generation, and multilingual tasks. It </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">also introduces the Mixtral 8x7B - Instruct model, which is fine-tuned to follow instructions and outperforms other</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">chat models on human evaluation benchmarks.'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>,\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m9\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m0.80\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'id'\u001b[0m: \u001b[91m3\u001b[0m, \u001b[92m'text'\u001b[0m: \u001b[92m'Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of \u001b[0m\n",
       "\u001b[92mthe 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In \u001b[0m\n",
       "\u001b[92mMixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates \u001b[0m\n",
       "\u001b[92msuperior capabilities in mathematics, code generation, and tasks that require multilingual understanding, \u001b[0m\n",
       "\u001b[92msignificantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully \u001b[0m\n",
       "\u001b[92mretrieve information from its context window of 32k tokens, regardless of the sequence length and the location of \u001b[0m\n",
       "\u001b[92mthe information in the sequence. We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow \u001b[0m\n",
       "\u001b[92minstructions using supervised fine-tuning and Direct Preference Optimization \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m25\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m. Its performance notably \u001b[0m\n",
       "\u001b[92msurpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation \u001b[0m\n",
       "\u001b[92mbenchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in \u001b[0m\n",
       "\u001b[92mbenchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â\\n\\nThis chunk describes the \u001b[0m\n",
       "\u001b[92mMixture of Experts layer architecture used in the Mixtral model, as well as the superior performance of Mixtral \u001b[0m\n",
       "\u001b[92mcompared to other models on various benchmarks, including mathematics, code generation, and multilingual tasks. It \u001b[0m\n",
       "\u001b[92malso introduces the Mixtral 8x7B - Instruct model, which is fine-tuned to follow instructions and outperforms other\u001b[0m\n",
       "\u001b[92mchat models on human evaluation benchmarks.'\u001b[0m, \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m,\n",
       "\u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">10</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.75</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">13</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"4 LLaMA 2 70B GPT-3.5 MMLU </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">MCQ in 57 subjects</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 69.9% 70.0% 70.6% </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">HellaSwag </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">10-shot</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 87.1% 85.5% 86.7% ARC Challenge </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">25-shot</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 85.1% 85.2% 85.8% WinoGrande </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">5-shot</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 83.2% 81.6% </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">81.2% MBPP </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">pass@1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 49.8% 52.2% 60.7% GSM-8K </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">5-shot</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 53.6% 57.1% 58.4% MT Bench </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">for Instruct Models</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 6.86 8.32 </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">8.30 # Mixtral 8x7B Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Llama 2 70B and GPT-3.5 performance on most metrics. Evaluation Differences. On some benchmarks, there are some </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">differences between our evaluation protocol and the one reported in the Llama 2 paper: 1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> on MBPP, we use the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">hand-verified subset 2</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> on TriviaQA, we do not provide Wikipedia contexts. # 3.1 Multilingual benchmarks Compared </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Table 4.\\n\\nThis chunk presents a comparison of the performance of Mixtral 8x7B, Llama 2 70B, and GPT-3.5 on </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">various benchmarks, as well as an analysis of Mixtral's performance on multilingual benchmarks.\"</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m10\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m0.75\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'id'\u001b[0m: \u001b[91m13\u001b[0m, \u001b[92m'text'\u001b[0m: \u001b[92m\"4 LLaMA 2 70B GPT-3.5 MMLU \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mMCQ in 57 subjects\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m 69.9% 70.0% 70.6% \u001b[0m\n",
       "\u001b[92mHellaSwag \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m10-shot\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m 87.1% 85.5% 86.7% ARC Challenge \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m25-shot\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m 85.1% 85.2% 85.8% WinoGrande \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m5-shot\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m 83.2% 81.6% \u001b[0m\n",
       "\u001b[92m81.2% MBPP \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mpass@1\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m 49.8% 52.2% 60.7% GSM-8K \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m5-shot\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m 53.6% 57.1% 58.4% MT Bench \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mfor Instruct Models\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m 6.86 8.32 \u001b[0m\n",
       "\u001b[92m8.30 # Mixtral 8x7B Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches \u001b[0m\n",
       "\u001b[92mLlama 2 70B and GPT-3.5 performance on most metrics. Evaluation Differences. On some benchmarks, there are some \u001b[0m\n",
       "\u001b[92mdifferences between our evaluation protocol and the one reported in the Llama 2 paper: 1\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m on MBPP, we use the \u001b[0m\n",
       "\u001b[92mhand-verified subset 2\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m on TriviaQA, we do not provide Wikipedia contexts. # 3.1 Multilingual benchmarks Compared \u001b[0m\n",
       "\u001b[92mto Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity\u001b[0m\n",
       "\u001b[92mallows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In \u001b[0m\n",
       "\u001b[92mparticular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in \u001b[0m\n",
       "\u001b[92mTable 4.\\n\\nThis chunk presents a comparison of the performance of Mixtral 8x7B, Llama 2 70B, and GPT-3.5 on \u001b[0m\n",
       "\u001b[92mvarious benchmarks, as well as an analysis of Mixtral's performance on multilingual benchmarks.\"\u001b[0m, \u001b[92m'metadata'\u001b[0m: \n",
       "\u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query the corpus\n",
    "sparse_results, sparse_scores = sparse_index.retrieve(query_tokens, k=10)\n",
    "\n",
    "for i in range(sparse_results.shape[1]):\n",
    "    doc, score = sparse_results[0, i], sparse_scores[0, i]\n",
    "    console.print(f\"Rank {i+1} (score: {score:.2f}): {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search - Dense Index\n",
    "\n",
    "For the Hybrid Search, we also need the dense index using the vector database, as we used in the previous steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creaing the Dense Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    \":memory:\"\n",
    ") \n",
    "\n",
    "# Create the embedding encoder\n",
    "dense_encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"hybrid_search\"\n",
    "\n",
    "dense_index = qdrant_client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "        size=dense_encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")\n",
    "print(dense_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize!\n",
    "qdrant_client.upload_points(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=dense_encoder.encode(doc[\"text\"]).tolist(),\n",
    "            payload=doc\n",
    "        ) for idx, doc in enumerate(corpus_json) # data is the variable holding all the enriched texts\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching the Dense Index\n",
    "\n",
    "We will start with encoding the query with the dense encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = dense_encoder.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the encoded query to search the dense index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_results = qdrant_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector,\n",
    "    limit=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">15</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">version</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">score</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.6180975806351034</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">15</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"3.8 â Mixtral_8x7B 3.5 32 &gt; $3.0 i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 &gt; 0.6 $3.0 i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Left</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Mixtral</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">sequence. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Right</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the proof-pile dataset decreases as the context length increases.\"</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">vector</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">shard_key</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">order_value</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">4</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">version</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">score</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.4972174713599332</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">4</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">details Mixtral is based on a transformer architecture </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">31</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> and uses the same modifications as described in </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">18</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward blocks are replaced by Mixture-of-Expert layers </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Section 2.1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. The model architecture parameters are </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mentions the model's open-source licensing and deployment options.\"</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">vector</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">shard_key</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">order_value</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">version</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">score</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.44239135249907124</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'expertsâ </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> to process the token and combine their output additively. This technique increases </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens.</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">2 70B and GPT-3.5 on various benchmarks.'</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">vector</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">shard_key</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">order_value</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">6</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">version</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">score</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.4404994080056008</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">6</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Table 1: Model architecture. # j nâ G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i Â· Ei</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">=0 Here, G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i denotes the n-dimensional </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">output of the gating network for the i-th expert, and Ei</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> is the output of the i-th expert network. If the gating</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">alternative ways of implementing G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">6, 15, 35</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, but a simple and performant one is implemented by taking the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">softmax over the Top-K logits of a linear layer </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">28</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">.\\n\\nThe chunk describes the architectural details of the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixtral model, specifically the Sparse Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> layer that is used in the model.'</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">vector</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">shard_key</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">order_value</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">7</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">version</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">score</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.42453512609457195</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">7</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'We use G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> := Softmax</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">TopK</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x Â· Wg</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">))</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, where </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">TopK</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">))</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i := â i if â i is among the top-K </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">coordinates of logits â â Rn and </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">TopK</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">))</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i := â â otherwise. The value of K â the number of experts used per </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">token â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">while keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mechanism used in the Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">MoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> layer of the Mixtral model. It explains how the router network </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">selects the top-K experts to process each token, and how this allows the model to increase its parameter count </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">while keeping the computational cost constant.'</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">vector</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">shard_key</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">order_value</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">42</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">version</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">score</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.3848539704676347</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">42</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'10 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">34</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">arXiv:2304.06364, 2023. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">35</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Quoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">which is relevant to the overall topic of the document discussing the Mixtral language model.'</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">vector</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">shard_key</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">order_value</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">45</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">version</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">score</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.3803753566702154</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">45</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">StackExchange â e-â Wikipedia </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">en</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">assignments occur a lot more often than they would with uniform assignments </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">materialized by the dashed lines</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Patterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">analysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">locality in the expert selection, especially at higher layers of the model. This analysis provides insights into </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the behavior of the sparse mixture-of-experts architecture used in Mixtral.'</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">vector</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">shard_key</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">order_value</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">version</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">score</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.3677008040909344</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'4 2 0 2 n a J 8 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> G L . s c </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> language model. Mixtral has the same </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i.e. experts</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">.</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">For every token, at each layer, a router network selects two experts to process the current state and combine their</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">vector</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">shard_key</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">order_value</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">5</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">version</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">score</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.36711093531341693</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">5</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">layer </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Figure 1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. For a more in-depth overview, see </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">12</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. The output of the MoE module for a given input x is </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">networkâ s output. i.e. given n expert networks </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">E0, Ei, ..., Enâ 1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">}</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, the output of the expert layer is given </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">by:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Experts layer that is a key component of the model.'</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">vector</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">shard_key</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">order_value</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">11</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">version</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">score</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.35405535832805446</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">11</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">â e LlaMA2 78 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">138 348 70B 7B </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">7B/8x7B</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> vs Llama 2 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">7B/13B/70B</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">and math. Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2.</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">benchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">against the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">reading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">metrics while using significantly fewer active parameters.'</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">vector</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">shard_key</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">order_value</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[93mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[91m15\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mversion\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mscore\u001b[0m=\u001b[91m0\u001b[0m\u001b[91m.6180975806351034\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m15\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m\"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length \u001b[0m\n",
       "\u001b[92mPasskey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K \u001b[0m\n",
       "\u001b[92m24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mLeft\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m Mixtral\u001b[0m\n",
       "\u001b[92mhas 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input \u001b[0m\n",
       "\u001b[92msequence. \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mRight\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length\u001b[0m\n",
       "\u001b[92mincreases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to \u001b[0m\n",
       "\u001b[92mretrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on \u001b[0m\n",
       "\u001b[92mthe proof-pile dataset decreases as the context length increases.\"\u001b[0m,\n",
       "            \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mvector\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mshard_key\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62morder_value\u001b[0m=\u001b[2;37mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[93mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[91m4\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mversion\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mscore\u001b[0m=\u001b[91m0\u001b[0m\u001b[91m.4972174713599332\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m4\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad\u001b[0m\n",
       "\u001b[92maccessibility and potential for diverse applications. To enable the community to run Mixtral with a fully \u001b[0m\n",
       "\u001b[92mopen-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient\u001b[0m\n",
       "\u001b[92minference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural \u001b[0m\n",
       "\u001b[92mdetails Mixtral is based on a transformer architecture \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m31\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m and uses the same modifications as described in \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m18\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m, \u001b[0m\n",
       "\u001b[92mwith the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- \u001b[0m\n",
       "\u001b[92mforward blocks are replaced by Mixture-of-Expert layers \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSection 2.1\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. The model architecture parameters are \u001b[0m\n",
       "\u001b[92msummarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including \u001b[0m\n",
       "\u001b[92mits use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also \u001b[0m\n",
       "\u001b[92mmentions the model's open-source licensing and deployment options.\"\u001b[0m,\n",
       "            \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mvector\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mshard_key\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62morder_value\u001b[0m=\u001b[2;37mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[93mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[91m2\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mversion\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mscore\u001b[0m=\u001b[91m0\u001b[0m\u001b[91m.44239135249907124\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m2\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m'expertsâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m to process the token and combine their output additively. This technique increases \u001b[0m\n",
       "\u001b[92mthe number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the \u001b[0m\n",
       "\u001b[92mtotal set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens.\u001b[0m\n",
       "\u001b[92mIt either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, \u001b[0m\n",
       "\u001b[92mMixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural \u001b[0m\n",
       "\u001b[92mdetails of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama \u001b[0m\n",
       "\u001b[92m2 70B and GPT-3.5 on various benchmarks.'\u001b[0m,\n",
       "            \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mvector\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mshard_key\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62morder_value\u001b[0m=\u001b[2;37mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[93mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[91m6\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mversion\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mscore\u001b[0m=\u001b[91m0\u001b[0m\u001b[91m.4404994080056008\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m6\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m'Table 1: Model architecture. # j nâ G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi Â· Ei\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. \u001b[0m\u001b[1;92mi\u001b[0m\u001b[92m=\u001b[0m\u001b[92m0\u001b[0m\u001b[92m Here, G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi denotes the n-dimensional \u001b[0m\n",
       "\u001b[92moutput of the gating network for the i-th expert, and Ei\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m is the output of the i-th expert network. If the gating\u001b[0m\n",
       "\u001b[92mvector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple \u001b[0m\n",
       "\u001b[92malternative ways of implementing G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m6, 15, 35\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m, but a simple and performant one is implemented by taking the \u001b[0m\n",
       "\u001b[92msoftmax over the Top-K logits of a linear layer \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m28\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m.\\n\\nThe chunk describes the architectural details of the \u001b[0m\n",
       "\u001b[92mMixtral model, specifically the Sparse Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m layer that is used in the model.'\u001b[0m,\n",
       "            \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mvector\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mshard_key\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62morder_value\u001b[0m=\u001b[2;37mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[93mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[91m7\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mversion\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mscore\u001b[0m=\u001b[91m0\u001b[0m\u001b[91m.42453512609457195\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m7\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m'We use G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m := Softmax\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mTopK\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx Â· Wg\u001b[0m\u001b[1;92m)\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m, where \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mTopK\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi := â i if â i is among the top-K \u001b[0m\n",
       "\u001b[92mcoordinates of logits â â Rn and \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mTopK\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi := â â otherwise. The value of K â the number of experts used per \u001b[0m\n",
       "\u001b[92mtoken â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n \u001b[0m\n",
       "\u001b[92mwhile keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating \u001b[0m\n",
       "\u001b[92mmechanism used in the Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m layer of the Mixtral model. It explains how the router network \u001b[0m\n",
       "\u001b[92mselects the top-K experts to process each token, and how this allows the model to increase its parameter count \u001b[0m\n",
       "\u001b[92mwhile keeping the computational cost constant.'\u001b[0m,\n",
       "            \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mvector\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mshard_key\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62morder_value\u001b[0m=\u001b[2;37mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[93mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[91m42\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mversion\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mscore\u001b[0m=\u001b[91m0\u001b[0m\u001b[91m.3848539704676347\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m42\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m'10 \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m34\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\u001b[0m\n",
       "\u001b[92mWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint \u001b[0m\n",
       "\u001b[92marXiv:2304.06364, 2023. \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m35\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, \u001b[0m\n",
       "\u001b[92mQuoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, \u001b[0m\n",
       "\u001b[92mwhich is relevant to the overall topic of the document discussing the Mixtral language model.'\u001b[0m,\n",
       "            \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mvector\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mshard_key\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62morder_value\u001b[0m=\u001b[2;37mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[93mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[91m45\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mversion\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mscore\u001b[0m=\u001b[91m0\u001b[0m\u001b[91m.3803753566702154\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m45\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- \u001b[0m\n",
       "\u001b[92mStackExchange â e-â Wikipedia \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92men\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated \u001b[0m\n",
       "\u001b[92massignments occur a lot more often than they would with uniform assignments \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mmaterialized by the dashed lines\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. \u001b[0m\n",
       "\u001b[92mPatterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the \u001b[0m\n",
       "\u001b[92manalysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal \u001b[0m\n",
       "\u001b[92mlocality in the expert selection, especially at higher layers of the model. This analysis provides insights into \u001b[0m\n",
       "\u001b[92mthe behavior of the sparse mixture-of-experts architecture used in Mixtral.'\u001b[0m,\n",
       "            \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mvector\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mshard_key\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62morder_value\u001b[0m=\u001b[2;37mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[93mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mversion\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mscore\u001b[0m=\u001b[91m0\u001b[0m\u001b[91m.3677008040909344\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m0\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m'4 2 0 2 n a J 8 \u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m G L . s c \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert \u001b[0m\n",
       "\u001b[92mQ. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh \u001b[0m\n",
       "\u001b[92mChaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, \u001b[0m\n",
       "\u001b[92mLÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon \u001b[0m\n",
       "\u001b[92mAntoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed \u001b[0m\n",
       "\u001b[92mAbstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m language model. Mixtral has the same \u001b[0m\n",
       "\u001b[92marchitecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mi.e. experts\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m.\u001b[0m\n",
       "\u001b[92mFor every token, at each layer, a router network selects two experts to process the current state and combine their\u001b[0m\n",
       "\u001b[92moutputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a\u001b[0m\n",
       "\u001b[92mresult, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was \u001b[0m\n",
       "\u001b[92mtrained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all \u001b[0m\n",
       "\u001b[92mevaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and \u001b[0m\n",
       "\u001b[92mmultilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that \u001b[0m\n",
       "\u001b[92msurpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse \u001b[0m\n",
       "\u001b[92mmixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes\u001b[0m\n",
       "\u001b[92mthe model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'\u001b[0m,\n",
       "            \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mvector\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mshard_key\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62morder_value\u001b[0m=\u001b[2;37mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[93mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[91m5\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mversion\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mscore\u001b[0m=\u001b[91m0\u001b[0m\u001b[91m.36711093531341693\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m5\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size \u001b[0m\n",
       "\u001b[92mnum_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts \u001b[0m\n",
       "\u001b[92mlayer \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mFigure 1\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. For a more in-depth overview, see \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m12\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m. The output of the MoE module for a given input x is \u001b[0m\n",
       "\u001b[92mdetermined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating \u001b[0m\n",
       "\u001b[92mnetworkâ s output. i.e. given n expert networks \u001b[0m\u001b[1;92m{\u001b[0m\u001b[92mE0, Ei, ..., Enâ 1\u001b[0m\u001b[1;92m}\u001b[0m\u001b[92m, the output of the expert layer is given \u001b[0m\n",
       "\u001b[92mby:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of \u001b[0m\n",
       "\u001b[92mExperts layer that is a key component of the model.'\u001b[0m,\n",
       "            \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mvector\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mshard_key\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62morder_value\u001b[0m=\u001b[2;37mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[93mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[91m11\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mversion\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mscore\u001b[0m=\u001b[91m0\u001b[0m\u001b[91m.35405535832805446\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m11\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138\u001b[0m\n",
       "\u001b[92m348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20\u001b[0m\n",
       "\u001b[92mâ e LlaMA2 78 \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m138 348 70B 7B \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: \u001b[0m\n",
       "\u001b[92mResults on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral \u001b[0m\n",
       "\u001b[1;92m(\u001b[0m\u001b[92m7B/8x7B\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m vs Llama 2 \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m7B/13B/70B\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading \u001b[0m\n",
       "\u001b[92mcomprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code \u001b[0m\n",
       "\u001b[92mand math. Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2.\u001b[0m\n",
       "\u001b[92mFigure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama\u001b[0m\n",
       "\u001b[92m2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics \u001b[0m\n",
       "\u001b[92mbenchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models \u001b[0m\n",
       "\u001b[92magainst the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, \u001b[0m\n",
       "\u001b[92mreading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most \u001b[0m\n",
       "\u001b[92mmetrics while using significantly fewer active parameters.'\u001b[0m,\n",
       "            \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mvector\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mshard_key\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62morder_value\u001b[0m=\u001b[2;37mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(dense_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search - Merging Results\n",
    "\n",
    "There are a few options to merge the results from the two methods (sparse and dense). In this notebook, we will use a simple weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_scores = []\n",
    "for hit in dense_results:\n",
    "    doc_id = hit.payload[\"id\"]\n",
    "    doc_text = next((doc for doc in corpus_json if doc[\"id\"] == doc_id), None)[\"text\"]\n",
    "    doc_dense_score = hit.score\n",
    "    documents_with_scores.append({\n",
    "        \"id\": doc_id,\n",
    "        \"text\": doc_text,\n",
    "        \"dense_score\": doc_dense_score\n",
    "    })\n",
    "\n",
    "for i, result in enumerate(sparse_results[0]):\n",
    "    doc_id = result[\"id\"]\n",
    "    doc_text = next((doc for doc in corpus_json if doc[\"id\"] == doc_id), None)[\"text\"]\n",
    "    doc_sparse_score = sparse_scores[0][i]\n",
    "    for doc in documents_with_scores:\n",
    "        if doc[\"id\"] == doc_id:\n",
    "            doc[\"sparse_score\"] = doc_sparse_score\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">15</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"3.8 â Mixtral_8x7B 3.5 32 &gt; $3.0 i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 &gt; 0.6 $3.0 i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Left</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Mixtral has 100% </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Right</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the proof-pile dataset decreases as the context length increases.\"</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.6180975806351034</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'sparse_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.333729</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">4</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">details Mixtral is based on a transformer architecture </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">31</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> and uses the same modifications as described in </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">18</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward blocks are replaced by Mixture-of-Expert layers </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Section 2.1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. The model architecture parameters are </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mentions the model's open-source licensing and deployment options.\"</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.4972174713599332</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'sparse_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.1242076</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'expertsâ </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> to process the token and combine their output additively. This technique increases the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">2 70B and GPT-3.5 on various benchmarks.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.44239135249907124</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'sparse_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.9919165</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">6</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Table 1: Model architecture. # j nâ G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i Â· Ei</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">=0 Here, G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i denotes the n-dimensional </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">output of the gating network for the i-th expert, and Ei</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> is the output of the i-th expert network. If the gating</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">alternative ways of implementing G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">6, 15, 35</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, but a simple and performant one is implemented by taking the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">softmax over the Top-K logits of a linear layer </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">28</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">.\\n\\nThe chunk describes the architectural details of the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixtral model, specifically the Sparse Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> layer that is used in the model.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.4404994080056008</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">7</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'We use G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> := Softmax</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">TopK</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x Â· Wg</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">))</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, where </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">TopK</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">))</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i := â i if â i is among the top-K </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">coordinates of logits â â Rn and </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">TopK</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">))</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i := â â otherwise. The value of K â the number of experts used per </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">token â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">while keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mechanism used in the Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">MoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> layer of the Mixtral model. It explains how the router network </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">selects the top-K experts to process each token, and how this allows the model to increase its parameter count </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">while keeping the computational cost constant.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.42453512609457195</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">42</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'10 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">34</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">arXiv:2304.06364, 2023. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">35</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Quoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">which is relevant to the overall topic of the document discussing the Mixtral language model.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.3848539704676347</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">45</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">StackExchange â e-â Wikipedia </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">en</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">assignments occur a lot more often than they would with uniform assignments </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">materialized by the dashed lines</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Patterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">analysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">locality in the expert selection, especially at higher layers of the model. This analysis provides insights into </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the behavior of the sparse mixture-of-experts architecture used in Mixtral.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.3803753566702154</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'4 2 0 2 n a J 8 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> G L . s c </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> language model. Mixtral has the same </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i.e. experts</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">.</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">For every token, at each layer, a router network selects two experts to process the current state and combine their</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.3677008040909344</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'sparse_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.3166082</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">5</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">layer </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Figure 1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. For a more in-depth overview, see </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">12</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. The output of the MoE module for a given input x is </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">networkâ s output. i.e. given n expert networks </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">E0, Ei, ..., Enâ 1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">}</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, the output of the expert layer is given </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">by:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Experts layer that is a key component of the model.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.36711093531341693</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">11</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">LlaMA2 78 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">138 348 70B 7B </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">7B/8x7B</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> vs </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Llama 2 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">7B/13B/70B</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">across most metrics. In particular, Mixtral displays a superior performance in code and mathematics </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">benchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">against the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">reading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">metrics while using significantly fewer active parameters.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.35405535832805446</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m15\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m\"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey \u001b[0m\n",
       "\u001b[92mPerformance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 \u001b[0m\n",
       "\u001b[92m5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mLeft\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m Mixtral has 100% \u001b[0m\n",
       "\u001b[92mretrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. \u001b[0m\n",
       "\u001b[1;92m(\u001b[0m\u001b[92mRight\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length \u001b[0m\n",
       "\u001b[92mincreases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to \u001b[0m\n",
       "\u001b[92mretrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on \u001b[0m\n",
       "\u001b[92mthe proof-pile dataset decreases as the context length increases.\"\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.6180975806351034\u001b[0m,\n",
       "        \u001b[92m'sparse_score'\u001b[0m: \u001b[91m1.333729\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m4\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad \u001b[0m\n",
       "\u001b[92maccessibility and potential for diverse applications. To enable the community to run Mixtral with a fully \u001b[0m\n",
       "\u001b[92mopen-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient\u001b[0m\n",
       "\u001b[92minference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural \u001b[0m\n",
       "\u001b[92mdetails Mixtral is based on a transformer architecture \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m31\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m and uses the same modifications as described in \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m18\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m, \u001b[0m\n",
       "\u001b[92mwith the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- \u001b[0m\n",
       "\u001b[92mforward blocks are replaced by Mixture-of-Expert layers \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSection 2.1\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. The model architecture parameters are \u001b[0m\n",
       "\u001b[92msummarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including \u001b[0m\n",
       "\u001b[92mits use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also \u001b[0m\n",
       "\u001b[92mmentions the model's open-source licensing and deployment options.\"\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.4972174713599332\u001b[0m,\n",
       "        \u001b[92m'sparse_score'\u001b[0m: \u001b[91m1.1242076\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m2\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'expertsâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m to process the token and combine their output additively. This technique increases the \u001b[0m\n",
       "\u001b[92mnumber of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total \u001b[0m\n",
       "\u001b[92mset of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It \u001b[0m\n",
       "\u001b[92meither matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, \u001b[0m\n",
       "\u001b[92mMixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural \u001b[0m\n",
       "\u001b[92mdetails of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama \u001b[0m\n",
       "\u001b[92m2 70B and GPT-3.5 on various benchmarks.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.44239135249907124\u001b[0m,\n",
       "        \u001b[92m'sparse_score'\u001b[0m: \u001b[91m1.9919165\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m6\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'Table 1: Model architecture. # j nâ G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi Â· Ei\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. \u001b[0m\u001b[1;92mi\u001b[0m\u001b[92m=\u001b[0m\u001b[92m0\u001b[0m\u001b[92m Here, G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi denotes the n-dimensional \u001b[0m\n",
       "\u001b[92moutput of the gating network for the i-th expert, and Ei\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m is the output of the i-th expert network. If the gating\u001b[0m\n",
       "\u001b[92mvector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple \u001b[0m\n",
       "\u001b[92malternative ways of implementing G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m6, 15, 35\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m, but a simple and performant one is implemented by taking the \u001b[0m\n",
       "\u001b[92msoftmax over the Top-K logits of a linear layer \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m28\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m.\\n\\nThe chunk describes the architectural details of the \u001b[0m\n",
       "\u001b[92mMixtral model, specifically the Sparse Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m layer that is used in the model.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.4404994080056008\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m7\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'We use G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m := Softmax\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mTopK\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx Â· Wg\u001b[0m\u001b[1;92m)\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m, where \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mTopK\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi := â i if â i is among the top-K \u001b[0m\n",
       "\u001b[92mcoordinates of logits â â Rn and \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mTopK\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi := â â otherwise. The value of K â the number of experts used per \u001b[0m\n",
       "\u001b[92mtoken â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n \u001b[0m\n",
       "\u001b[92mwhile keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating \u001b[0m\n",
       "\u001b[92mmechanism used in the Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m layer of the Mixtral model. It explains how the router network \u001b[0m\n",
       "\u001b[92mselects the top-K experts to process each token, and how this allows the model to increase its parameter count \u001b[0m\n",
       "\u001b[92mwhile keeping the computational cost constant.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.42453512609457195\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m42\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'10 \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m34\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, \u001b[0m\n",
       "\u001b[92mWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint \u001b[0m\n",
       "\u001b[92marXiv:2304.06364, 2023. \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m35\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, \u001b[0m\n",
       "\u001b[92mQuoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, \u001b[0m\n",
       "\u001b[92mwhich is relevant to the overall topic of the document discussing the Mixtral language model.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.3848539704676347\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m45\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- \u001b[0m\n",
       "\u001b[92mStackExchange â e-â Wikipedia \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92men\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated \u001b[0m\n",
       "\u001b[92massignments occur a lot more often than they would with uniform assignments \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mmaterialized by the dashed lines\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. \u001b[0m\n",
       "\u001b[92mPatterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the \u001b[0m\n",
       "\u001b[92manalysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal \u001b[0m\n",
       "\u001b[92mlocality in the expert selection, especially at higher layers of the model. This analysis provides insights into \u001b[0m\n",
       "\u001b[92mthe behavior of the sparse mixture-of-experts architecture used in Mixtral.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.3803753566702154\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m0\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'4 2 0 2 n a J 8 \u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m G L . s c \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. \u001b[0m\n",
       "\u001b[92mJiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
       "\u001b[92mDiego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio \u001b[0m\n",
       "\u001b[92mRenard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon \u001b[0m\n",
       "\u001b[92mAntoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed \u001b[0m\n",
       "\u001b[92mAbstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m language model. Mixtral has the same \u001b[0m\n",
       "\u001b[92marchitecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mi.e. experts\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m.\u001b[0m\n",
       "\u001b[92mFor every token, at each layer, a router network selects two experts to process the current state and combine their\u001b[0m\n",
       "\u001b[92moutputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a\u001b[0m\n",
       "\u001b[92mresult, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was \u001b[0m\n",
       "\u001b[92mtrained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all \u001b[0m\n",
       "\u001b[92mevaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and \u001b[0m\n",
       "\u001b[92mmultilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that \u001b[0m\n",
       "\u001b[92msurpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse \u001b[0m\n",
       "\u001b[92mmixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes\u001b[0m\n",
       "\u001b[92mthe model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.3677008040909344\u001b[0m,\n",
       "        \u001b[92m'sparse_score'\u001b[0m: \u001b[91m1.3166082\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m5\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size \u001b[0m\n",
       "\u001b[92mnum_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts \u001b[0m\n",
       "\u001b[92mlayer \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mFigure 1\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. For a more in-depth overview, see \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m12\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m. The output of the MoE module for a given input x is \u001b[0m\n",
       "\u001b[92mdetermined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating \u001b[0m\n",
       "\u001b[92mnetworkâ s output. i.e. given n expert networks \u001b[0m\u001b[1;92m{\u001b[0m\u001b[92mE0, Ei, ..., Enâ 1\u001b[0m\u001b[1;92m}\u001b[0m\u001b[92m, the output of the expert layer is given \u001b[0m\n",
       "\u001b[92mby:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of \u001b[0m\n",
       "\u001b[92mExperts layer that is a key component of the model.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.36711093531341693\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m11\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348\u001b[0m\n",
       "\u001b[92m70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e\u001b[0m\n",
       "\u001b[92mLlaMA2 78 \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m138 348 70B 7B \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results\u001b[0m\n",
       "\u001b[92mon MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m7B/8x7B\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m vs \u001b[0m\n",
       "\u001b[92mLlama 2 \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m7B/13B/70B\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension \u001b[0m\n",
       "\u001b[92mbenchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. \u001b[0m\n",
       "\u001b[92mDetailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 \u001b[0m\n",
       "\u001b[92mcompares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B \u001b[0m\n",
       "\u001b[92macross most metrics. In particular, Mixtral displays a superior performance in code and mathematics \u001b[0m\n",
       "\u001b[92mbenchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models \u001b[0m\n",
       "\u001b[92magainst the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, \u001b[0m\n",
       "\u001b[92mreading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most \u001b[0m\n",
       "\u001b[92mmetrics while using significantly fewer active parameters.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.35405535832805446\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(documents_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize the scores of each index, and than calculate a weighted score that gives more weight (0.8) to the dense index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize the two types of scores\n",
    "dense_scores = np.array([doc.get(\"dense_score\", 0) for doc in documents_with_scores])\n",
    "sparse_scores = np.array([doc.get(\"sparse_score\", 0) for doc in documents_with_scores])\n",
    "\n",
    "dense_scores_normalized = (dense_scores - np.min(dense_scores)) / (np.max(dense_scores) - np.min(dense_scores))\n",
    "sparse_scores_normalized = (sparse_scores - np.min(sparse_scores)) / (np.max(sparse_scores) - np.min(sparse_scores))\n",
    "\n",
    "# Calculate a weighted score with alpha of 0.2 to the sparse score\n",
    "alpha = 0.2\n",
    "weighted_scores = (1 - alpha) * dense_scores_normalized + alpha * sparse_scores_normalized\n",
    "\n",
    "# Pick up the top 3 documents with the weighted score\n",
    "top_docs = sorted(\n",
    "    zip(\n",
    "        documents_with_scores, \n",
    "        weighted_scores\n",
    "    ), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True\n",
    ")[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">15</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"3.8 â Mixtral_8x7B 3.5 32 &gt; $3.0 i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 &gt; 0.6 $3.0 i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Left</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Mixtral</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">sequence. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Right</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the proof-pile dataset decreases as the context length increases.\"</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.6180975806351034</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'sparse_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.333729</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.9339141478808913</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">4</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">details Mixtral is based on a transformer architecture </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">31</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> and uses the same modifications as described in </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">18</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward blocks are replaced by Mixture-of-Expert layers </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Section 2.1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. The model architecture parameters are </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mentions the model's open-source licensing and deployment options.\"</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.4972174713599332</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'sparse_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.1242076</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.5466321931956223</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'expertsâ </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> to process the token and combine their output additively. This technique increases </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens.</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">2 70B and GPT-3.5 on various benchmarks.'</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.44239135249907124</span>,\n",
       "            <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'sparse_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.9919165</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.4676420260341326</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m15\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m\"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length \u001b[0m\n",
       "\u001b[92mPasskey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K \u001b[0m\n",
       "\u001b[92m24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mLeft\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m Mixtral\u001b[0m\n",
       "\u001b[92mhas 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input \u001b[0m\n",
       "\u001b[92msequence. \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mRight\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length\u001b[0m\n",
       "\u001b[92mincreases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to \u001b[0m\n",
       "\u001b[92mretrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on \u001b[0m\n",
       "\u001b[92mthe proof-pile dataset decreases as the context length increases.\"\u001b[0m,\n",
       "            \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.6180975806351034\u001b[0m,\n",
       "            \u001b[92m'sparse_score'\u001b[0m: \u001b[91m1.333729\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[91m0.9339141478808913\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m4\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad\u001b[0m\n",
       "\u001b[92maccessibility and potential for diverse applications. To enable the community to run Mixtral with a fully \u001b[0m\n",
       "\u001b[92mopen-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient\u001b[0m\n",
       "\u001b[92minference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural \u001b[0m\n",
       "\u001b[92mdetails Mixtral is based on a transformer architecture \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m31\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m and uses the same modifications as described in \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m18\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m, \u001b[0m\n",
       "\u001b[92mwith the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- \u001b[0m\n",
       "\u001b[92mforward blocks are replaced by Mixture-of-Expert layers \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSection 2.1\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. The model architecture parameters are \u001b[0m\n",
       "\u001b[92msummarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including \u001b[0m\n",
       "\u001b[92mits use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also \u001b[0m\n",
       "\u001b[92mmentions the model's open-source licensing and deployment options.\"\u001b[0m,\n",
       "            \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.4972174713599332\u001b[0m,\n",
       "            \u001b[92m'sparse_score'\u001b[0m: \u001b[91m1.1242076\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[91m0.5466321931956223\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[92m'id'\u001b[0m: \u001b[91m2\u001b[0m,\n",
       "            \u001b[92m'text'\u001b[0m: \u001b[92m'expertsâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m to process the token and combine their output additively. This technique increases \u001b[0m\n",
       "\u001b[92mthe number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the \u001b[0m\n",
       "\u001b[92mtotal set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens.\u001b[0m\n",
       "\u001b[92mIt either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, \u001b[0m\n",
       "\u001b[92mMixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural \u001b[0m\n",
       "\u001b[92mdetails of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama \u001b[0m\n",
       "\u001b[92m2 70B and GPT-3.5 on various benchmarks.'\u001b[0m,\n",
       "            \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.44239135249907124\u001b[0m,\n",
       "            \u001b[92m'sparse_score'\u001b[0m: \u001b[91m1.9919165\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[91m0.4676420260341326\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using merged results to generate a reply\n",
    "\n",
    "We can now take the merged results and call the LLM to generate the reply to the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable to hold the search results for the generation model\n",
    "search_results = [doc[0]['text'] for doc in top_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to connect to the large language model\n",
    "from openai import OpenAI\n",
    "from rich.text import Text\n",
    "\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are chatbot, an research expert. Your top priority is to help guide users to understand reserach papers.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "        {\"role\": \"assistant\", \"content\": str(search_results)}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response_text = Text(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────── Hybrid Search Reply to \"What is context size of Mixtral?\" ───────────────────────────╮\n",
       "│ The context size of Mixtral is 32,000 tokens.                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────── Hybrid Search Reply to \"What is context size of Mixtral?\" ───────────────────────────╮\n",
       "│ The context size of Mixtral is 32,000 tokens.                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.panel import Panel\n",
    "\n",
    "panel = Panel(response_text, title=f\"Hybrid Search Reply to \\\"{query}\\\"\")\n",
    "console.print(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the retrieved documents to be used in the next reranking notebook, which demonstrates a more advanced method to merge Hybrid Search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/dense_results.json', 'w') as f:\n",
    "    json.dump([dense_result.payload for dense_result in dense_results], f, default=str)\n",
    "\n",
    "with open('data/sparse_results.json', 'w') as f:\n",
    "    json.dump([sparse_result for sparse_result in sparse_results[0]], f, default=str)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
